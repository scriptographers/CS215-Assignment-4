\documentclass[11pt, fleqn]{article}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{mathtools}
\usepackage{hyperref}
\usepackage{ulem}
\usepackage{enumitem}
\usepackage[left=0.75in, right=0.75in, bottom=0.75in]{geometry}
% \usepackage{float}
\usepackage{floatrow}
\usepackage{graphicx}
\usepackage[export]{adjustbox}

\usepackage{sectsty}
\sectionfont{\centering}

\usepackage[perpage]{footmisc}

\usepackage{fancyhdr}
\pagestyle{fancy}
\fancyhf{}
\lhead{190100044 \& 190100055}
\rhead{CS 215: Assignment 4}
\renewcommand{\footrulewidth}{1.0pt}
\cfoot{Page \thepage}

\setlength{\parindent}{0em}
\renewcommand{\arraystretch}{2}%

\title{Assignment 4: CS 215}
\author{
\begin{tabular}{|c|c|}
     \hline
     Devansh Jain & Harshit Varma \\
     \hline
     190100044 & 190100055 \\
     \hline
\end{tabular}
}
\date{\today}

\begin{document}

\maketitle
\tableofcontents
\thispagestyle{empty}
\setcounter{page}{0}

\renewcommand{\arraystretch}{1}

\newpage
\section*{Question 1}
\addcontentsline{toc}{section}{Question 1}
\setcounter{equation}{0}
\setcounter{figure}{0}

\subsection*{(a)}
$X$ can take all values inside the unit square of area $4$ with equal probability.\\
Thus, the probability that $X$ takes values inside an arbitrary region of area $A$ (lying inside the unit square) is proportional to $A$.\\
$$
    P(X \in R) = k\cdot area(R)
$$
As the probability that $X$ lies inside the unit square is 1, this gives $k = \frac{1}{4}$.\\
A unit circle has an area of $\pi$, thus the probability that $X$ lies inside a unit circle is $\boxed{\frac{\pi}{4}}$

\subsection*{(b)}
Estimation of $\pi$ using $X$:\\
Generate $n$ samples of the form $(x_1, x_2)$ where  $x_1, x_2 \thicksim U(-1, 1)$.\\
Find the number of such points which satisfy $x_1^2 + x_2^2 \le 1$, let this number be $n_u$\\
Thus, $\frac{n_u}{n} \approx \frac{\pi}{4}$, an estimate of $\pi$ will be $\frac{4n_u}{n}$\\
More formally, let $I_{||X||\le 1}(X)$ be the indicator function that yields 1 if $||X|| = \sqrt{X_1^2 + X_2^2}\le 1$, and is 0 otherwise.\\
Then, $n_u = \sum_{i}^n I_{||x_i||\le 1}(x_i)$, thus an estimate for $\pi$ will be $\frac{4\sum_{i}^n I_{||x_i||\le 1}(x_i)}{n}$

\subsection*{(c)}
Estimates of $\pi$ obtained for $N = 10, 10^2, 10^3, 10^4, 10^5, 10^6, 10^7, 10^8$ respectively
\begin{verbatim}
    N = 10, pi = 2.0000 
    N = 100, pi = 3.2400 
    N = 1000, pi = 3.2080 
    N = 10000, pi = 3.1316 
    N = 100000, pi = 3.1425 
    N = 1000000, pi = 3.1419 
    N = 10000000, pi = 3.1414 
    N = 100000000, pi = 3.1416 
\end{verbatim}
Our code handles $N = 10^9$ by computing $n_u$ using 10 $10^8$ (Similarly $\frac{n}{10^8}$ iterations for a general $n$, assuming it's a power of 10) sized arrays, by iterating 10 times, to ensure memory remains in check, although this takes quite a bit of time to execute. We get the output for $N = 10^9$ as \texttt{pi = 3.1416}
\begin{verbatim}
    n_fixed = single(10^8);
    n_large = single(10^9);
    n_iters = single(n_large/n_fixed);
    
    n_u = single(0);
    for i = 1:n_iters
        X1 = single(2*rand(n_fixed, 1)-1);
        X2 = single(2*rand(n_fixed, 1)-1);
        n_u = n_u + sum((X1.^2 + X2.^2) <= 1);
    end
\end{verbatim}

\subsection*{(d)}
$n_u = \sum_{i}^n I_{||x_i||\le 1}(x_i)$, also $y_i = I_{||x_i||\le 1}(x_i)$ is a Bernoulli random variable with the parameter $p = \frac{\pi}{4}$.\\
Let $z = \frac{1}{n}\sum_{i=1}^{n}I_{||x_i||\le 1}(x_i)$ (Thus, an estimate of $\pi$ will be $4z$)\\
Note that $\{y_i\}_{i=1}^n$ are IID RVs\\
Let $var(y_i) = \sigma^2$, $E(y_i) = \mu$\\
Then, by the central limit theorem, the RV $w = \sqrt{n}\frac{(z - \mu)}{\sigma}$ is approximately $\thicksim \mathcal{N}(0, 1)$\\
We know that $\mu = \pi/4$ and $\sigma = \sqrt{(\pi/4)(1-\pi/4)}$ thus, (We know both of these exactly as we are allowed to assume that we know $\pi$ exactly)\\
We need the smallest n such that $P(\pi - 0.01 \le 4z \le \pi + 0.01) = 0.95$\\
$$
    \begin{aligned}
        P(\pi - 0.01 \le 4z \le \pi + 0.01) &= P((\pi - 0.01)/4 \le z \le (\pi + 0.01)/4)\\
        &= P(- 0.01/4 \le z-\mu \le  0.01/4)\\
        &= P(-0.01\sqrt{n}/4 \le \sqrt{n}(z-\mu) \le 0.01\sqrt{n}/4)\\
        &= P\left(-\frac{0.01\sqrt{n}}{4\sigma} \le w \le \frac{0.01\sqrt{n}}{4\sigma} \right) = 0.95
    \end{aligned}
$$
Thus, we need a $95\%$ confidence interval of $w$\\
We know that the 95\% confidence interval for the standard normal distribution is the interval $(-1.96, 1.96)$\\
Thus,
$$
    \begin{aligned}
        \frac{0.01\sqrt{n}}{4\sigma} &= 1.96\\
        n &= (196\times4\sigma)^2 \\ 
        &= \left(196\times4\sqrt{\frac{\pi(4-\pi)}{16}}\right)^2\\
        &\approx \boxed{103599}
    \end{aligned}
$$

\subsection*{Instructions for running the code:}
\begin{enumerate}[itemsep=-1ex]
    \item Unzip and \texttt{cd} to \texttt{code}, under this find the file named \texttt{q1.m}
    \item On running, it will print the estimates of $\pi$ for the given values of $n$, and it will also print the estimate for $\pi$ for $n = 10^9$ (This may take some time)
\end{enumerate}





\newpage
\section*{Question 2}
\addcontentsline{toc}{section}{Question 2}
\setcounter{equation}{0}
\setcounter{figure}{0}

\newpage
\section*{Question 3}
\addcontentsline{toc}{section}{Question 3}
\setcounter{equation}{0}
\setcounter{figure}{0}
We have been given observed set of data $\{ \mathbf{z}_i \}_{i=1}^{N}$, where $\mathbf{z}_i = (x_i, y_i)$\\
We need to find a linear relationship between the RVs $X$ and $Y$

\medskip
Thus, we need to find $m, c$ such that $Y \approx mX + c$ using PCA.\\
This is essentially dimensionality reduction from $\mathbb{R}^2$ to $\mathbb{R}$\\
Thus, we need to find the first principal mode of variance/principal component of $\{ \mathbf{z}_i \}_{i=1}^{N}$

\medskip
Let the sample mean be $\mu = \bigg(\mu_1 = \frac{\sum_{i=1}^{N}x_i}{N},\  \mu_2 = \frac{\sum_{i=1}^{N}y_i}{N}\bigg)$, then first we center the data about $\mu$.\\
Let the sample covariance matrix of the new centered data be $C$.\\
Since we are not allowed to use \texttt{cov()} and \texttt{mean()}, we compute the sample $\mu = \frac{\sum_{i=1}^{N}\mathbf{z_i}}{N}$.\\
Let $Z = [\mathbf{z_i} - \mu]$ be a $N\times2$ matrix of the entire \textbf{mean-centered} observed data.\\
Then $C = \frac{Z^T Z}{N-1}$ will give us the covariance matrix.

\medskip
On eigenvalue decomposition of $C$, we get $C = Q\Lambda Q^T$, let $\mathbf{d}$ correspond to the diagonal values of $\Lambda$, let $j = \text{argmax}_{i: 1\le i\le 2} \ \mathbf{d}$, then the $j^{th}$ column of Q corresponds to the principal mode of variation, $\mathbf{v} = (v_1, v_2)$

\medskip
Thus, $m = \frac{v_2}{v_1}$, and $y = mx$, since this was the centered coordinate system (about $\mu$), we need to shift back to the original coordinate system, thus, the final straight line will be $y-\mu_2 = m(x - \mu_1)$

\medskip
The same method as above has been implemented in \texttt{q3.m}

\medskip
In this case, PCA chooses the direction along which the variance of the projected data is maximized.\\
The quality of the approximation worsens with the given data deviating from a linear relationship (when we want to fit a straight line through the data).\\
In the worst case, consider the data distributed around a circle.\\
In this case, all the directions through the mean will have approximately the same variance of the projected data, thus there is no ``best" direction.\\
In the first case, there appears to be a linear relationship in the given data, thus reducing the dimension to 1 using PCA yields good quality results.\\
In the second case, the data is distributed in an approximate ellipse, thus the major axis will be the direction which maximizes the variance of the projected data, however, in case of an ellipse, there is considerable variance along the minor axis too, PCA approximation loses this information about the original data and thus the quality of results is poor compared to the first set of data.\\
Thus, trying to reduce the dimension of the data to 1, in data which varies along more than one direction is thus meaningless in most of the cases.

\subsection*{Instructions for running the code:}
\begin{enumerate}[itemsep=-1ex]
    \item Unzip and \texttt{cd} to \texttt{code}, under this find the file named \texttt{q3.m}
    \item Find the variable \texttt{PATH\_TO\_LOAD} on the $7^{th}$ line, change this to \texttt{DATA\_PATH\_1} or \texttt{DATA\_PATH\_2} accordingly, run the file, a scatter plot with a fitted line will generated and saved to the \texttt{plots} folder.
\end{enumerate}

\newpage
\section*{Question 4}
\addcontentsline{toc}{section}{Question 4}
\setcounter{equation}{0}
\setcounter{figure}{0}

\newpage
\section*{Question 5}
\addcontentsline{toc}{section}{Question 5}
\setcounter{equation}{0}
\setcounter{figure}{0}

\newpage
\section*{Question 6}
\addcontentsline{toc}{section}{Question 6}
\setcounter{equation}{0}
\setcounter{figure}{0}
\subsection*{Closest Representation:}
Let $d = 80\times80\times3$\\
Let the sample mean of the original data be $\mu \in \mathbb{R}^d$, the sample covariance matrix of the original data be $C \in \mathbb{R}^{d\times d}$\\
Let the 4 chosen (normalized) eigenvectors of $C$ be $e_1, e_2, e_3, e_4 \in \mathbb{R}^d$\\ 
Let the original images (uncentered) be $\{x_i\}_{i=1}^N$, in this case $N = 16$\\
Then, the projection of $x_i$ onto the new dimensions will be given by $\sum_{j=1}^{4}((x_i - \mu)\cdot e_j)e_j$.\\
Now, since the projections are in a mean centered space, to come back into the uncentred space, we need to add $\mu$ to get the reconstruction of the original image:
$$
    \boxed{r_i = \mu + \sum_{j=1}^{4}((x_i - \mu)\cdot e_j)e_j}
$$
Thus, the coefficients used in the linear combination are $\boxed{c_{ij} = (x_i - \mu)\cdot e_j}$, now we need to prove that these coefficients indeed minimize the frobenius norm, which is equivalent to minimizing the square of the frobenius norm.\\

For the $i^{th}$ unrolled image, square of the frobinius norm is given by:
$$ 
    f_i^2 = ||x_i - r_i||^2 
$$

Note that $x_i$ is \textbf{exactly} equal to $ \mu + \sum_{j=1}^{d}((x_i - \mu)\cdot e_j)e_j $ (Sum of projections over \textbf{all} the eigenvectors of $C$), thus:
$$
\begin{aligned}
    f_i^2 &= \left|\left|\mu + \sum_{j=1}^{d}((x_i - \mu)\cdot e_j)e_j - (\mu + \sum_{j=1}^{4}((x_i - \mu)\cdot e_j)e_j)\right|\right|^2 \\
    &= \left|\left|\sum_{j=1}^{d}((x_i - \mu)\cdot e_j)e_j - \sum_{j=1}^{4}((x_i - \mu)\cdot e_j)e_j\right|\right|^2\\
    &= \left|\left|\sum_{j=5}^{d}((x_i - \mu)\cdot e_j)e_j\right|\right|^2 = \left|\left|\sum_{j=5}^{d}c_{ij}e_j\right|\right|^2\\
    &\text{Since any pair of $e_j$s are orthogonal, and $e_j$ are unit vectors,}\\
    f_i^2 &= \left|\left|\sum_{j=5}^{d}c_{ij}e_j\right|\right|^2 = \sum_{j=5}^{d}c_{ij}^2 = \sum_{j=5}^{d}((x_i - \mu)\cdot e_j)^2\\ 
\end{aligned}
$$
Thus we need to \textbf{minimize} $ \sum_{j=5}^{d}((x_i - \mu)\cdot e_j)^2$\\
Now, the variance of the projected data which PCA \textbf{maximizes} is $\sum_{j=1}^{4}((x_i - \mu)\cdot e_j)^2$\\
Since, 
$$ 
\begin{aligned}
    x_i &= \mu + \sum_{j=1}^{d}((x_i - \mu)\cdot e_j)e_j\\ 
    (x_i - \mu)^2 &= (\sum_{j=1}^{d}((x_i - \mu)\cdot e_j)e_j)^2 \\
    &\text{Since any pair of $e_j$s are orthogonal, and $e_j$ are unit vectors,}\\
    &= \sum_{j=1}^{d}((x_i - \mu)\cdot e_j)^2 = \sum_{j=1}^{4}((x_i - \mu)\cdot e_j)^2 + \sum_{j=5}^{d}((x_i - \mu)\cdot e_j)^2\\
    \Aboxed{(x_i - \mu)^2 &= constant =  \sum_{j=1}^{4}((x_i - \mu)\cdot e_j)^2 + \sum_{j=5}^{d}((x_i - \mu)\cdot e_j)^2} 
\end{aligned}
$$
Thus, \textbf{maximizing} the variance of the projected data: $\sum_{j=1}^{4}((x_i - \mu)\cdot e_j)^2$ is same as \textbf{minimizing} the frobenius norm of the difference: $\sum_{j=5}^{d}((x_i - \mu)\cdot e_j)^2 = f_i^2 $.
 
\medskip
Thus, in effect, PCA minimizes the frobenius norm of the difference between the projected and the actual image.\\
We follow the same way to reconstruct the images in our algorithm.\\
Let $E = [e_1\  e_2\  e_3 \ e_4]^T \in \mathbb{R}^{d \times 4} $, let $X \in \mathbb{R}^{d \times 16}$ contain the mean centered data, then $A = X^TE \in \mathbb{R}^{16 \times 4}$, where $A_{ij} = c_{ij}$, and thus the reconstructed data $\hat X = \mu + EA^T \in \mathbb{R}^{d \times 16}$, here $\mu$ is added to each column of $EA^T$.\\
In MATLAB:
\begin{verbatim}
    % data is the mean centered data 
    % Q4 contains the 4 eigenvectors, coeffs contain the required coefficients 
    coeffs = data' * Q4; % 16x4
    data_reconstructed = mu + Q4*coeffs';
\end{verbatim}

\subsection*{Generating new images:}
For generating new images, we need to sample data in the reduced domain and then project it back into the original domain.\\
In the projected domain, each point looks like $\mathbf{c} = [c_1\  c_2\  c_3\  c_4]^T \in \mathbb{R}^4$, to project this back into the original domain ($\mathbb{R}^d$):
$$
\begin{aligned}
    x_{gen} &= \mu + \sum_{j=1}^{4}c_j\mathbf{e_j}\\
\end{aligned}
$$
Thus, we need to sample $\mathbf{c}$.\\
We assume that $c \thicksim \mathcal{N}(\mathbf{0}, C)$ (A 0-mean multivariate gaussian with covariance matrix $C \in \mathbb{R}^{4\times 4}$)\\
We take $C = diag([\lambda_1, \lambda_2, \lambda_3, \lambda_4])$, since the variance of the original data along the chosen eigenvectors is given by the corresponding eigenvalues.\\
Now we sample random vectors $\mathbf{c}$ from this multivariate gaussian, project back to the original space and plot the images.\\
We achieve the following results which are different, but representative of the dataset:

\end{document}